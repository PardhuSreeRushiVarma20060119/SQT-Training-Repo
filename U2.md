# UNIT-2: SOFTWARE TESTING
## A Comprehensive Guide to Test Planning, Scenarios, and Test Cases

---

## 1️⃣ SOFTWARE TESTING — FOUNDATIONAL CONTEXT

### 1.1 Definition and Purpose

**Software Testing** is a systematic verification and validation discipline that evaluates whether a software system meets specified requirements and behaves correctly under defined conditions. It serves as a critical quality gate within the Software Development Life Cycle (SDLC).

### 1.2 Core Principles

**Testing as a Verification and Validation Discipline:**
- **Verification**: "Are we building the product right?" — Confirms the software conforms to specifications
- **Validation**: "Are we building the right product?" — Ensures the software meets user needs and business objectives

**Testing as a Risk-Reduction Mechanism:**
- Identifies defects before production deployment
- Reduces the probability of system failure in operational environments
- Minimizes financial, reputational, and safety-related risks
- Provides stakeholders with confidence in system quality

**Testing as a Quality Gate:**
- Acts as a decision checkpoint in SDLC phases
- Determines readiness for production release
- Enforces entry and exit criteria for each phase
- Prevents defect propagation to downstream phases

### 1.3 Critical Distinctions

| Aspect | Testing | Debugging | Quality Assurance |
|--------|---------|-----------|-------------------|
| **Purpose** | Find defects through execution | Fix identified defects | Prevent defects through process |
| **Activity Type** | Verification/Validation | Correction | Process improvement |
| **Scope** | Product evaluation | Code-level investigation | Entire SDLC |
| **Performer** | Test Engineers | Developers | QA Team |
| **Output** | Test results, defect reports | Fixed code | Process standards, audits |

**Testing ≠ Debugging:**
- Testing reveals the presence of defects through systematic execution
- Debugging locates and removes the root cause of defects
- Testing is a quality measurement activity; debugging is a corrective activity

**Testing ≠ Quality Assurance:**
- QA is process-oriented (preventing defects through better processes)
- Testing is product-oriented (detecting defects through execution)
- QA encompasses testing, but testing alone does not constitute QA

**Testing as Evidence Generation:**
- Testing generates objective evidence of system behavior
- Test results provide measurable data for quality assessment
- Testing documents proof of requirement compliance
- Focus is on systematic evaluation, not merely "finding bugs"

---

## 2️⃣ TEST PLAN — IEEE 829 STANDARD

### 2.1 Definition and Significance

**A Test Plan** is a comprehensive document that defines the scope, approach, resources, schedule, and activities required to conduct testing for a software system. It serves as the strategic blueprint for all testing activities.

**Why IEEE 829 Exists:**
- Provides international standardization for test documentation
- Ensures consistency across organizations and projects
- Facilitates communication among stakeholders
- Establishes a template for completeness and traceability
- Reduces ambiguity through structured format
- Supports audit and compliance requirements

**Standard Reference:** IEEE 829-2008 (later revised as IEEE 829-1998, superseded by IEEE/ISO/IEC 29119 series, but IEEE 829 structure remains foundational)

### 2.2 IEEE 829 Test Plan Components (Detailed)

---

#### **1. Test Plan Identifier**

**Purpose:** Uniquely identifies the test plan document within the organization's documentation system.

**Content:**
- Unique alphanumeric identifier (e.g., `TP-PROJ-001-V1.0`)
- Version number
- Project/product name reference

**What Goes Wrong If Omitted:**
- Document confusion in multi-project environments
- Version control issues
- Inability to trace test results to specific test plan versions

**Example:**
```
Test Plan ID: TP-EBANK-LOGIN-2024-V2.1
Project: e-Banking System
Module: User Authentication
Version: 2.1
Date: January 5, 2026
```

---

#### **2. Introduction**

**Purpose:** Provides context, background, and high-level overview of the testing effort.

**Content:**
- Project background
- Document purpose and scope summary
- Intended audience
- References to related documents (Requirements Specification, Design Documents)
- Definitions, acronyms, and abbreviations

**What Goes Wrong If Omitted:**
- Stakeholders lack context for testing decisions
- Misalignment between testing and project objectives
- Communication gaps among team members

**Example:**
```
This test plan defines the testing strategy for the User Authentication Module 
of the e-Banking System. The system allows customers to securely access their 
accounts via username/password and two-factor authentication. This document is 
intended for QA engineers, developers, project managers, and business analysts.

Related Documents:
- SRS-EBANK-AUTH-001 (Requirements Specification)
- DD-EBANK-AUTH-002 (Design Document)
```

---

#### **3. Test Items**

**Purpose:** Explicitly identifies the software components, modules, or features that are subject to testing.

**Content:**
- List of software items (modules, features, interfaces)
- Version numbers of test items
- References to specification documents
- Dependencies and prerequisites

**What Goes Wrong If Omitted:**
- Scope creep or incomplete coverage
- Testing of wrong versions
- Confusion about what needs verification

**Example:**
```
Test Items:
1. Login Module v3.2
   - Username/Password Authentication
   - Session Management
   - Password Reset Functionality
2. Two-Factor Authentication Module v1.5
   - OTP Generation and Validation
   - SMS Gateway Integration
3. User Profile API v2.0
   
Specification Reference: SRS-EBANK-AUTH-001, Section 4.1-4.3
```

---

#### **4. Features to be Tested**

**Purpose:** Lists the specific functionalities and characteristics that will undergo testing.

**Content:**
- Functional requirements to be verified
- Non-functional characteristics (performance, security, usability)
- Integration points
- Priority classification (critical, high, medium, low)

**What Goes Wrong If Omitted:**
- Important features overlooked
- No basis for test case derivation
- Misalignment with stakeholder expectations

**Example:**
```
Features to be Tested:

Functional:
- F1: User login with valid credentials (Priority: Critical)
- F2: Password complexity validation (Priority: High)
- F3: Account lockout after failed attempts (Priority: Critical)
- F4: "Remember Me" functionality (Priority: Medium)
- F5: Password reset via email (Priority: High)

Non-Functional:
- NF1: Login response time < 2 seconds (Priority: High)
- NF2: Password encryption using AES-256 (Priority: Critical)
- NF3: SQL injection prevention (Priority: Critical)
```

---

#### **5. Features Not to be Tested**

**Purpose:** Explicitly documents what is **out of scope** for the current testing cycle.

**Content:**
- Features deferred to future releases
- Third-party components covered by vendor testing
- Previously tested and stable components
- Justification for exclusion

**What Goes Wrong If Omitted:**
- Stakeholder assumptions about complete coverage
- Wasted effort testing out-of-scope items
- Unclear accountability boundaries

**Example:**
```
Features NOT to be Tested:

1. Social Media Login Integration (Facebook/Google)
   Reason: Deferred to Release 2.0
   
2. Biometric Authentication
   Reason: Hardware dependency; requires specialized testing environment
   
3. Email Server Functionality
   Reason: Third-party service (SendGrid) with independent certification
   
4. Database Connection Pooling
   Reason: Infrastructure component tested in previous release; no changes
```

---

#### **6. Test Approach / Strategy**

**Purpose:** Defines the overall testing methodology, levels, types, and techniques to be employed.

**Content:**
- Testing levels (Unit, Integration, System, Acceptance)
- Testing types (Functional, Performance, Security, Usability)
- Testing techniques (Black-box, White-box, Gray-box)
- Test design techniques to be applied
- Entry and exit criteria for each test level
- Test automation scope (if any, though Unit-2 focuses on manual)

**What Goes Wrong If Omitted:**
- Inconsistent testing execution
- No systematic coverage strategy
- Inability to estimate effort and schedule

**Example:**
```
Test Approach:

1. Testing Levels:
   - System Testing: Primary focus for this plan
   - Integration Testing: API-Database interaction
   
2. Testing Types:
   - Functional Testing: 70% effort
   - Security Testing: 20% effort (authentication vulnerabilities)
   - Performance Testing: 10% effort (login response time)
   
3. Test Design Techniques:
   - Equivalence Partitioning for input validation
   - Boundary Value Analysis for password length, retry attempts
   - Decision Table Testing for lockout logic
   - State Transition Testing for session management
   
4. Entry Criteria:
   - Requirements approved and baselined
   - Test environment provisioned and accessible
   - Test data prepared
   
5. Exit Criteria:
   - 100% critical test cases executed
   - No open critical/high severity defects
   - 95% test case pass rate
```

---

#### **7. Item Pass/Fail Criteria**

**Purpose:** Establishes objective thresholds that determine whether a test item has passed or failed testing.

**Content:**
- Criteria for individual test cases
- Criteria for feature-level acceptance
- Criteria for overall test cycle completion
- Defect severity thresholds

**What Goes Wrong If Omitted:**
- Subjective or inconsistent quality judgments
- Disputes about readiness for release
- No clear milestone completion indicators

**Example:**
```
Pass/Fail Criteria:

Test Case Level:
- Pass: Actual result matches expected result exactly
- Fail: Any deviation from expected result

Feature Level:
- Pass: All critical test cases pass; ≤5% non-critical failures
- Fail: Any critical test case fails

Test Cycle Completion:
- Pass Criteria:
  * 100% of critical features pass
  * 95% of high-priority features pass
  * Zero open Severity-1 (Critical) defects
  * ≤3 open Severity-2 (High) defects with approved workarounds
  * Test coverage ≥90% of requirements
  
- Fail Criteria:
  * Any critical security vulnerability unresolved
  * >10% test case failure rate
  * Any feature causing data corruption
```

---

#### **8. Suspension and Resumption Criteria**

**Purpose:** Defines conditions under which testing must be halted and conditions for resuming testing.

**Content:**
- Events triggering test suspension
- Required corrective actions
- Conditions for safe test resumption
- Notification and escalation procedures

**What Goes Wrong If Omitted:**
- Wasted effort testing unstable builds
- Safety risks in continued testing
- No formal restart protocol

**Example:**
```
Suspension Criteria:
Testing will be suspended if:

S1: Build Stability
- >30% test cases fail due to environment issues
- Critical functionality (login) completely non-functional

S2: Blocking Defects
- Severity-1 defect prevents further test execution
- Data corruption observed

S3: Resource Unavailability
- Test environment down for >4 hours
- Essential test data unavailable

Resumption Criteria:
Testing may resume when:

R1: Build stability restored and smoke test passes (10 critical test cases)
R2: Blocking defects fixed and verified
R3: Environment restored and validated
R4: Test Manager approval obtained
R5: Impact analysis completed for partial test execution

Notification: Test Lead notifies Project Manager and stakeholders within 2 hours 
of suspension decision via email and project dashboard update.
```

---

#### **9. Test Deliverables**

**Purpose:** Lists all documents, reports, and artifacts produced during the testing process.

**Content:**
- Test plan document
- Test case specifications
- Test execution logs
- Defect reports
- Test summary reports
- Traceability matrices
- Test data sets
- Test environment specifications

**What Goes Wrong If Omitted:**
- Incomplete documentation trail
- Audit and compliance failures
- Knowledge loss and inability to reproduce results

**Example:**
```
Test Deliverables:

Before Testing:
1. Test Plan Document (this document)
2. Test Case Specification Document
3. Requirement Traceability Matrix (RTM)
4. Test Data Preparation Document

During Testing:
5. Daily Test Execution Status Reports
6. Defect Reports (logged in JIRA)
7. Test Logs and Screenshots

After Testing:
8. Test Summary Report
9. Defect Summary Report
10. Test Metrics Dashboard
11. Lessons Learned Document

Delivery Schedule:
- Daily Reports: End of each testing day
- Test Summary: Within 2 business days of cycle completion
- Final Report: 1 week after release
```

---

#### **10. Testing Tasks**

**Purpose:** Breaks down testing activities into discrete, manageable tasks with dependencies.

**Content:**
- Task identification and description
- Task dependencies and sequencing
- Effort estimation
- Task ownership
- Milestones

**What Goes Wrong If Omitted:**
- Uncoordinated testing efforts
- Missed dependencies causing rework
- No basis for progress tracking

**Example:**
```
Testing Tasks:

Task ID | Task Description                        | Depends On | Owner        | Effort | Status
--------|----------------------------------------|------------|--------------|--------|--------
T1      | Test environment setup                 | -          | DevOps Team  | 2 days | Done
T2      | Test data preparation                  | T1         | QA Lead      | 1 day  | Done
T3      | Test case design (Functional)          | -          | QA Team      | 3 days | In Progress
T4      | Test case review and approval          | T3         | QA Manager   | 1 day  | Pending
T5      | Smoke testing                          | T1, T2     | QA Engineer1 | 0.5 day| Pending
T6      | Functional test execution              | T4, T5     | QA Team      | 5 days | Pending
T7      | Security test execution                | T5         | Security QA  | 2 days | Pending
T8      | Defect retesting                       | T6, T7     | QA Team      | 2 days | Pending
T9      | Test summary report preparation        | T8         | QA Lead      | 1 day  | Pending

Milestones:
- M1: Test preparation complete (T1-T4) - Day 7
- M2: First test cycle complete (T6-T7) - Day 14
- M3: Testing complete (T8-T9) - Day 17
```

---

#### **11. Environmental Needs**

**Purpose:** Specifies all hardware, software, network, and infrastructure requirements for testing.

**Content:**
- Hardware specifications
- Software and tools
- Network configuration
- Test data requirements
- Special equipment or licenses
- Access credentials and permissions

**What Goes Wrong If Omitted:**
- Test execution delays due to missing resources
- Environment inconsistencies causing false defects
- Budget overruns from unplanned procurements

**Example:**
```
Environmental Needs:

Hardware:
- 3 Windows 10 Pro workstations (16GB RAM, i5 processor)
- 2 Ubuntu Linux servers for test environment
- Mobile devices: iPhone 12, Samsung Galaxy S21

Software:
- Windows 10 Professional (Licensed)
- Browsers: Chrome v120, Firefox v121, Edge v120
- Database: PostgreSQL 14.5
- Test Management Tool: TestRail (5 user licenses)
- Defect Tracking: JIRA (5 user licenses)

Network:
- Isolated test network (10.20.30.0/24)
- VPN access for remote testers
- Bandwidth: Minimum 50 Mbps
- Firewall rules configured for test server access

Test Data:
- 100 test user accounts (roles: admin, user, guest)
- 500 sample transaction records
- PII data anonymized per privacy policy

Access Requirements:
- Admin credentials for test database
- API authentication tokens
- Source code repository read access (GitHub)

Special Requirements:
- SMS gateway sandbox account (for OTP testing)
- Email server (SMTP test server)
```

---

#### **12. Responsibilities**

**Purpose:** Clearly assigns roles and accountability for testing activities.

**Content:**
- Role definitions
- Individual/team assignments
- Authority and decision-making powers
- Communication protocols
- Escalation paths

**What Goes Wrong If Omitted:**
- Confusion about ownership
- Accountability gaps
- Decision-making paralysis
- Overlapping or neglected duties

**Example:**
```
Responsibilities:

Role: Test Manager (Sarah Johnson)
- Overall test planning and strategy
- Resource allocation and budget management
- Stakeholder communication
- Risk management and escalation decisions
- Final approval of test deliverables

Role: Test Lead (Michael Chen)
- Test case design supervision
- Daily test execution coordination
- Defect triage and prioritization
- Test environment management
- Daily status reporting

Role: QA Engineers (3 members: Alex, Maria, David)
- Test case development
- Test execution (manual)
- Defect logging and verification
- Test data preparation
- Test result documentation

Role: Security QA Specialist (Priya Sharma)
- Security test case design
- Vulnerability testing
- Security defect verification
- Security compliance validation

Role: Business Analyst (John Davis)
- Requirements clarification
- Test case review from business perspective
- User acceptance criteria validation

Role: DevOps Engineer (Carlos Martinez)
- Test environment provisioning and maintenance
- Build deployment to test environment
- Environment issue resolution

Escalation Path:
Level 1: Test Lead → Test Manager (within 4 hours)
Level 2: Test Manager → Project Manager (within 8 hours)
Level 3: Project Manager → Program Director (within 24 hours)
```

---

#### **13. Staffing and Training Needs**

**Purpose:** Identifies human resource requirements and skill development needs.

**Content:**
- Number of personnel required
- Skill level requirements
- Training programs needed
- Onboarding plans
- Knowledge transfer mechanisms

**What Goes Wrong If Omitted:**
- Insufficient team capacity
- Skill gaps causing quality issues
- Training delays impacting schedule
- Knowledge silos

**Example:**
```
Staffing and Training Needs:

Staffing Requirements:

Position            | Count | Skills Required                    | Duration
--------------------|-------|------------------------------------|---------
Test Manager        | 1     | 5+ years QA, ISTQB Advanced       | Full project
Test Lead           | 1     | 3+ years QA, domain knowledge     | Full project
QA Engineers        | 3     | 1-2 years QA, SQL, test design    | Full project
Security QA         | 1     | Security testing, OWASP Top 10    | 2 weeks
Performance Tester  | 1     | Load testing tools, metrics       | 1 week

Training Needs:

Training Program                    | Participants     | Duration | Schedule
------------------------------------|------------------|----------|------------
Banking Domain Fundamentals         | All QA Engineers | 2 days   | Week 1
Test Design Techniques (BVA, EP)    | All QA Engineers | 1 day    | Week 1
JIRA Defect Management              | All QA Engineers | 0.5 day  | Week 1
Security Testing Basics             | All QA Engineers | 1 day    | Week 2
IEEE 829 Test Documentation         | Test Lead        | 0.5 day  | Week 1

Knowledge Transfer:
- Daily standup meetings (15 minutes)
- Weekly technical knowledge sharing sessions
- Test case review meetings (bi-weekly)
- Documentation repository maintained in Confluence

External Resources:
- ISTQB certification preferred but not mandatory
- Consultant for security testing (if needed): Budget approved
```

---

#### **14. Schedule**

**Purpose:** Establishes timeline for all testing phases and activities.

**Content:**
- Test phase start and end dates
- Major milestones
- Task durations
- Dependencies and critical path
- Buffer time for contingencies

**What Goes Wrong If Omitted:**
- No project timeline alignment
- Resource scheduling conflicts
- Unrealistic expectations about delivery
- No basis for progress measurement

**Example:**
```
Testing Schedule:

Phase                          | Start Date  | End Date    | Duration | Status
-------------------------------|-------------|-------------|----------|--------
Test Planning                  | Jan 8, 2026 | Jan 14, 2026| 5 days   | Completed
Test Environment Setup         | Jan 10, 2026| Jan 12, 2026| 3 days   | Completed
Test Design                    | Jan 15, 2026| Jan 21, 2026| 5 days   | In Progress
Test Case Review               | Jan 22, 2026| Jan 23, 2026| 2 days   | Pending
Test Execution - Cycle 1       | Jan 24, 2026| Jan 30, 2026| 5 days   | Pending
Defect Fixing & Retesting      | Jan 31, 2026| Feb 4, 2026 | 3 days   | Pending
Test Execution - Cycle 2       | Feb 5, 2026 | Feb 7, 2026 | 3 days   | Pending
Regression Testing             | Feb 10, 2026| Feb 12, 2026| 3 days   | Pending
Test Closure Activities        | Feb 13, 2026| Feb 14, 2026| 2 days   | Pending

Key Milestones:
- M1: Test Plan Approved          : Jan 14, 2026
- M2: Test Cases Approved         : Jan 23, 2026
- M3: First Cycle Complete        : Jan 30, 2026
- M4: All Critical Defects Closed : Feb 7, 2026
- M5: Testing Sign-Off            : Feb 14, 2026

Critical Path: Test Design → Cycle 1 Execution → Defect Fixing → Cycle 2 → Sign-off

Contingency: 2-day buffer allocated between Feb 12-14 for unexpected issues
```

---

#### **15. Risks and Contingencies**

**Purpose:** Identifies potential threats to testing success and defines mitigation strategies.

**Content:**
- Risk identification and assessment
- Probability and impact analysis
- Mitigation strategies
- Contingency plans
- Risk monitoring approach

**What Goes Wrong If Omitted:**
- Surprised by predictable problems
- No proactive risk management
- Project delays and budget overruns
- Quality compromises under pressure

**Example:**
```
Risks and Contingencies:

Risk ID | Risk Description                  | Probability | Impact | Risk Level
--------|-----------------------------------|-------------|--------|------------
R1      | Requirement changes mid-testing   | High        | High   | CRITICAL
R2      | Test environment downtime         | Medium      | High   | HIGH
R3      | Key tester unavailability         | Medium      | Medium | MEDIUM
R4      | Third-party API unavailable       | Low         | High   | MEDIUM
R5      | Insufficient test data            | Low         | Medium | LOW

Risk Mitigation and Contingency Plans:

R1: Requirement Changes
- Mitigation: Implement change control board; freeze requirements post-baseline
- Contingency: Maintain 20% schedule buffer; prioritize critical features
- Monitoring: Weekly requirement review meetings

R2: Test Environment Downtime
- Mitigation: Daily environment health checks; backup environment provisioned
- Contingency: Switch to backup environment within 2 hours; work on test design during downtime
- Monitoring: Automated monitoring with alerts

R3: Key Tester Unavailability
- Mitigation: Cross-training; documentation of critical knowledge
- Contingency: Redistribute workload; engage backup resource from resource pool
- Monitoring: Track leave calendar; maintain skill matrix

R4: Third-Party API Issues
- Mitigation: API mock/stub environment for testing; early integration testing
- Contingency: Test other features; coordinate with vendor for priority support
- Monitoring: Daily API health check; vendor communication channel

R5: Insufficient Test Data
- Mitigation: Test data requirements defined early; data generation scripts
- Contingency: Synthetic data generation; use production data snapshots (anonymized)
- Monitoring: Data coverage review in test preparation phase

Risk Review: Weekly risk review in project status meetings; risk register updated
```

---

#### **16. Approvals**

**Purpose:** Documents formal authorization to proceed with testing per this plan.

**Content:**
- Approval signatures
- Dates of approval
- Authority levels
- Conditions of approval
- Version control information

**What Goes Wrong If Omitted:**
- No formal commitment from stakeholders
- Disputes about agreed scope
- Lack of organizational buy-in
- No audit trail for accountability

**Example:**
```
Approvals:

This test plan has been reviewed and approved by the undersigned:

Role                    | Name              | Signature | Date
------------------------|-------------------|-----------|-------------
Test Manager            | Sarah Johnson     | [Signed]  | Jan 14, 2026
Project Manager         | Robert Williams   | [Signed]  | Jan 14, 2026
Development Manager     | Emily Anderson    | [Signed]  | Jan 14, 2026
Business Analyst        | John Davis        | [Signed]  | Jan 14, 2026
Quality Assurance Mgr   | Lisa Thompson     | [Signed]  | Jan 14, 2026

Conditions of Approval:
1. Security testing scope to be expanded based on threat assessment (due Jan 16)
2. Performance testing baseline to be established in first cycle
3. Monthly status reports to steering committee

Document Version Control:
- Version: 2.1
- Previous Version: 2.0 (dated Jan 10, 2026)
- Changes: Added security QA resource; updated schedule based on environment delays
- Next Review: At 50% test execution completion or on significant scope change

Approval indicates agreement with:
- Scope and objectives defined herein
- Resource commitments and schedule
- Budget allocation for testing activities
- Risk acceptance and mitigation strategies
```

---

## 3️⃣ CREATING A TEST PLAN — PROCESS VIEW

### 3.1 The Test Planning Lifecycle

Test plan creation is not a one-time document generation activity but a systematic, phased process integrated into the SDLC. The process follows a structured workflow with clear inputs, activities, and outputs at each stage.

---

### 3.2 Step-by-Step Process

#### **Step 1: Requirement Analysis**

**Purpose:** Thoroughly understand what needs to be tested and establish the foundation for test planning.

**Activities:**
1. Review requirements documents (SRS, FRS, User Stories)
2. Identify functional and non-functional requirements
3. Analyze requirement clarity, completeness, and testability
4. Identify ambiguities and gaps; log clarification requests
5. Participate in requirement review meetings
6. Understand business objectives and user expectations
7. Identify regulatory and compliance requirements

**Key Questions:**
- What is the system supposed to do?
- What quality attributes are critical?
- Are requirements measurable and verifiable?
- What are the acceptance criteria?

**Stakeholder Involvement:**
- Business Analysts (requirement clarification)
- Product Owners (priority guidance)
- Subject Matter Experts (domain knowledge)

**Entry Criteria:**
- Requirements document available
- Access granted to requirements repository

**Exit Criteria:**
- All requirements understood
- Testable requirements identified
- Clarifications documented

**Deliverable:** Requirements Analysis Report / Testability Assessment

---

#### **Step 2: Scope Definition**

**Purpose:** Establish clear boundaries for what will and will not be included in testing.

**Activities:**
1. Categorize requirements by priority (Critical/High/Medium/Low)
2. Define features to be tested
3. Explicitly identify features NOT to be tested (with justification)
4. Determine test levels (unit, integration, system, acceptance)
5. Determine test types (functional, performance, security, usability)
6. Identify integration points and external dependencies
7. Define test item boundaries (modules, components, APIs)
8. Establish test environment scope

**Key Decisions:**
- Which features are in-scope for this release/cycle?
- What is the depth of testing for each feature?
- Which third-party components require testing vs. acceptance?
- What is deferred to future cycles?

**Stakeholder Involvement:**
- Project Manager (schedule and budget constraints)
- Development Manager (technical feasibility)
- Test Manager (testing capacity)

**Entry Criteria:**
- Requirement analysis complete
- Project scope and objectives defined

**Exit Criteria:**
- Test scope documented and agreed
- Scope exclusions justified and documented

**Deliverable:** Test Scope Document (often incorporated into Test Plan Sections 3-5)

---

#### **Step 3: Risk Identification**

**Purpose:** Proactively identify threats to testing effectiveness and overall quality.

**Activities:**
1. Conduct risk identification workshop with stakeholders
2. Analyze technical risks (complexity, new technology, integration points)
3. Analyze project risks (schedule, resources, dependencies)
4. Analyze quality risks (critical features, high-defect areas)
5. Assess risk probability and impact
6. Prioritize risks using risk matrix
7. Define risk mitigation strategies
8. Define contingency plans for high-impact risks

**Risk Categories:**
- **Product Risks:** Quality issues in critical features
- **Project Risks:** Schedule delays, resource unavailability
- **Technical Risks:** Technology complexity, integration challenges
- **External Risks:** Third-party dependencies, regulatory changes

**Risk Assessment Matrix:**

| Probability / Impact | Low | Medium | High |
|---------------------|-----|--------|------|
| **High**            | Med | High   | Critical |
| **Medium**          | Low | Med    | High |
| **Low**             | Low | Low    | Med |

**Stakeholder Involvement:**
- All team members (risk identification)
- Technical Leads (technical risk assessment)
- Project Manager (project risk assessment)
- Test Manager (quality risk assessment)

**Entry Criteria:**
- Test scope defined
- Team assembled

**Exit Criteria:**
- Risk register created
- High/Critical risks have mitigation plans

**Deliverable:** Risk Register / Risk Analysis Report (becomes Test Plan Section 15)

---

#### **Step 4: Test Strategy Selection**

**Purpose:** Define the overall approach, methodology, and techniques for conducting testing.

**Activities:**
1. Select test levels appropriate for the project
2. Select test types based on risk and requirements
3. Determine test design techniques (BVA, EP, Decision Tables, etc.)
4. Define test execution approach (manual, automated, hybrid)
5. Establish entry and exit criteria for each test phase
6. Define defect management process
7. Determine test data strategy
8. Define test environment strategy
9. Establish metrics and reporting approach
10. Select tools (test management, defect tracking)

**Strategy Considerations:**
- Project size and complexity
- Team skillset and experience
- Schedule and budget constraints
- Regulatory and compliance requirements
- Risk profile

**Test Strategy Components:**
```
Example Test Strategy Summary:

Approach: Risk-Based Testing
Primary Level: System Testing (black-box)
Primary Types: Functional (70%), Security (20%), Performance (10%)
Techniques: Equivalence Partitioning, Boundary Value Analysis, Decision Tables
Execution: Manual testing (this cycle)
Test Data: Synthetic data generated via scripts + 10 production scenarios
Environment: Dedicated test environment mirroring production
Tools: TestRail (test management), JIRA (defect tracking)
Metrics: Requirements coverage, defect density, test execution progress
```

**Stakeholder Involvement:**
- Test Manager (strategy definition)
- Development Manager (technical alignment)
- Project Manager (feasibility review)

**Entry Criteria:**
- Scope and risks identified
- Team skillset assessed

**Exit Criteria:**
- Test strategy documented and reviewed
- Strategy approved by stakeholders

**Deliverable:** Test Strategy Document (becomes Test Plan Section 6)

---

#### **Step 5: Resource and Environment Planning**

**Purpose:** Ensure all necessary resources—human, hardware, software, and infrastructure—are identified and allocated.

**Activities:**

**Resource Planning:**
1. Determine staffing needs (number, roles, skills)
2. Identify training requirements
3. Assign roles and responsibilities
4. Create RACI matrix (Responsible, Accountable, Consulted, Informed)
5. Plan for knowledge transfer and onboarding
6. Identify external resource needs (consultants, vendors)

**Environment Planning:**
1. Define hardware requirements
2. Define software and tool requirements
3. Define network and infrastructure requirements
4. Identify test data requirements
5. Define access and permission requirements
6. Plan for environment setup and configuration
7. Establish environment maintenance procedures

**Schedule Planning:**
1. Identify all testing tasks
2. Estimate effort for each task
3. Determine task dependencies
4. Create testing schedule with milestones
5. Identify critical path
6. Allocate buffer time
7. Align with overall project schedule

**Budget Planning:**
1. Estimate costs (personnel, tools, infrastructure)
2. Secure budget approval
3. Plan for contingency budget

**Stakeholder Involvement:**
- Test Manager (resource planning)
- DevOps/Infrastructure Team (environment planning)
- Project Manager (schedule and budget)
- HR/Resource Manager (staffing)

**Entry Criteria:**
- Test strategy defined
- Project schedule available

**Exit Criteria:**
- Resources identified and committed
- Environment requirements documented
- Schedule baseline established
- Budget approved

**Deliverables:** 
- Resource Plan (Test Plan Sections 12-13)
- Environment Plan (Test Plan Section 11)
- Test Schedule (Test Plan Section 14)
- Budget Estimate

---

#### **Step 6: Review and Approval Flow**

**Purpose:** Ensure the test plan is accurate, complete, feasible, and has organizational commitment before execution begins.

**Activities:**

**Internal Review (QA Team):**
1. Test Lead reviews for completeness and consistency
2. Senior QA engineers review for technical accuracy
3. Peer review for clarity and adherence to standards
4. Self-audit against IEEE 829 checklist
5. Incorporate feedback and revise document

**Stakeholder Review:**
1. Distribute test plan to all stakeholders
2. Conduct formal review meeting
3. Present test strategy, scope, schedule, and risks
4. Address questions and concerns
5. Document review comments and action items
6. Revise test plan based on feedback

**Formal Approval:**
1. Obtain sign-off from Test Manager
2. Obtain sign-off from Project Manager
3. Obtain sign-off from Development Manager
4. Obtain sign-off from Business/Product Owner
5. Obtain sign-off from QA Manager
6. Document approvals with signatures and dates
7. Version control and baseline the approved test plan
8. Distribute final approved version to all stakeholders

**Review Checklist (Sample Questions):**
- Are all IEEE 829 components present?
- Is test scope clearly defined and justified?
- Are risks identified with mitigation plans?
- Are resources adequate and committed?
- Is the schedule realistic and achievable?
- Are responsibilities clearly assigned?
- Are entry/exit criteria measurable?
- Are deliverables clearly defined?

**Stakeholder Involvement:**
- All stakeholders listed in Approvals section
- Test Manager (facilitates review)
- Configuration Management (version control)

**Entry Criteria:**
- Test plan draft complete
- All prior steps completed

**Exit Criteria:**
- All review comments addressed
- Formal approvals obtained
- Baseline version published

**Deliverable:** Approved and Baselined Test Plan Document

---

### 3.3 Post-Approval Activities

#### **Test Plan Maintenance:**
The test plan is a living document that may require updates during the project lifecycle.

**Triggers for Test Plan Updates:**
- Requirement changes
- Scope changes
- Schedule changes
- Resource changes
- Risk profile changes
- Major defects affecting strategy

**Change Control Process:**
1. Identify need for change
2. Document proposed change with justification
3. Impact analysis (scope, schedule, resources, risks)
4. Review with stakeholders
5. Obtain approvals
6. Update test plan with version increment
7. Communicate changes to all stakeholders
8. Update baseline

**Version Control:**
- Major changes: Increment major version (e.g., 2.0 → 3.0)
- Minor changes: Increment minor version (e.g., 2.1 → 2.2)
- Maintain change history log in document

---

### 3.4 Stakeholder Roles Throughout Test Planning

| Stakeholder | Requirement Analysis | Scope Definition | Risk Identification | Strategy Selection | Resource Planning | Review & Approval |
|-------------|---------------------|------------------|---------------------|-------------------|-------------------|-------------------|
| **Test Manager** | Lead | Lead | Participate | Lead | Lead | Approve |
| **Project Manager** | Consult | Participate | Participate | Review | Participate | Approve |
| **Business Analyst** | Lead | Participate | Consult | Consult | Inform | Approve |
| **Dev Manager** | Consult | Participate | Participate | Review | Participate | Approve |
| **QA Engineers** | Participate | Inform | Participate | Consult | Inform | Inform |
| **Product Owner** | Consult | Participate | Inform | Inform | Inform | Approve |

**Legend:**
- **Lead:** Primary responsibility and decision-making authority
- **Participate:** Active involvement and contribution
- **Consult:** Provide input and expertise when requested
- **Review:** Evaluate and provide feedback
- **Inform:** Kept updated on decisions and progress
- **Approve:** Formal authorization required

---

### 3.5 Entry and Exit Criteria for Test Planning Phase

#### **Entry Criteria (To Begin Test Planning):**
1. Requirements document available and baselined
2. Project charter and objectives defined
3. Test Manager assigned
4. Access to project artifacts and stakeholders
5. Initial project schedule available
6. High-level scope and budget defined

#### **Exit Criteria (Test Planning Complete):**
1. Test plan document complete per IEEE 829
2. All 16 sections documented
3. Internal review completed
4. Stakeholder review completed
5. All review comments addressed
6. Formal approvals obtained
7. Test plan baselined and distributed
8. Kick-off meeting conducted
9. Team understands their roles and responsibilities
10. Test environment procurement initiated

---

## 4️⃣ TEST SCENARIOS — ABSTRACT TEST THINKING

### 4.1 Definition

**A Test Scenario** is a high-level description of what to test from an end-user perspective. It represents a testable functionality or business flow without detailing the specific steps or data required to execute the test.

**Key Characteristics:**
- **High-Level:** Describes "what" to test, not "how" to test
- **Requirement-Driven:** Directly derived from functional requirements or user stories
- **User-Centric:** Written from the end-user's or business perspective
- **Broad Coverage:** One scenario can encompass multiple test cases
- **Non-Executable:** Cannot be directly executed without further elaboration

**Analogy:** If test cases are the detailed travel itinerary with step-by-step directions, test scenarios are the high-level travel goals like "Visit Paris" or "Explore the Eiffel Tower."

---

### 4.2 Purpose and Benefits

**Why Test Scenarios Exist:**

1. **Early Test Planning:** Can be created during requirements phase before detailed design
2. **Communication Tool:** Bridge between business stakeholders and technical testers
3. **Coverage Assessment:** Quick way to verify all requirements have test coverage
4. **Test Case Derivation:** Foundation for creating detailed test cases
5. **Scope Clarity:** Helps define what will be tested at a glance
6. **Stakeholder Review:** Easier for non-technical stakeholders to review and validate
7. **Traceability:** Maps requirements to testing activities at a conceptual level

---

### 4.3 Characteristics of Well-Written Test Scenarios

| Characteristic | Description | Example |
|---------------|-------------|---------|
| **Clear** | Unambiguous and easy to understand | ✓ "Verify user login with valid credentials"<br>✗ "Check login" |
| **Concise** | Brief, typically one sentence | ✓ "Verify password reset via email"<br>✗ "Verify that when user forgets password..." |
| **User-Focused** | Written from user's perspective | ✓ "User updates profile information"<br>✗ "API validates profile data" |
| **Testable** | Represents a verifiable condition | ✓ "Verify account lockout after failed attempts"<br>✗ "System should be secure" |
| **Requirement-Mapped** | Traceable to specific requirement | Each scenario linked to SRS section |
| **Complete** | Covers end-to-end business flow | Includes all major user actions for feature |
| **Input-Agnostic** | No specific test data mentioned | ✓ "Login with valid credentials"<br>✗ "Login with username='admin'" |

---

### 4.4 Test Scenario Structure

**Basic Format:**
```
Scenario ID: TS-<Module>-<Number>
Scenario: [Action] + [Object] + [Condition]
Requirement ID: [Reference to requirement]
Priority: [Critical/High/Medium/Low]
```

**Example:**
```
Scenario ID: TS-AUTH-001
Scenario: Verify user login with valid credentials
Requirement ID: REQ-AUTH-101
Priority: Critical
```

---

### 4.5 Creating Test Scenarios: Step-by-Step

**Step 1: Analyze Requirements**
- Read and understand each functional requirement
- Identify testable functionalities

**Step 2: Identify User Workflows**
- Map out how users interact with the system
- Consider different user roles and permissions

**Step 3: Write Scenario Statements**
- Use action verbs: Verify, Validate, Check, Ensure, Test
- Keep scenarios independent and atomic
- Focus on user-facing functionality

**Step 4: Review for Coverage**
- Ensure every requirement has at least one scenario
- Check for gaps in functionality coverage
- Validate with stakeholders

**Step 5: Prioritize Scenarios**
- Assign priority based on business criticality and risk
- Consider regulatory and compliance requirements

---

### 4.6 Comprehensive Example: Login Module

**Context:** E-Banking System - User Authentication Module

**Requirements Reference:**
- REQ-AUTH-101: System shall authenticate users with username and password
- REQ-AUTH-102: System shall enforce password complexity rules
- REQ-AUTH-103: System shall lock account after 3 consecutive failed login attempts
- REQ-AUTH-104: System shall provide password reset functionality via email
- REQ-AUTH-105: System shall maintain user session for 15 minutes of inactivity
- REQ-AUTH-106: System shall provide logout functionality

---

**Test Scenarios for Login Module:**

| Scenario ID | Test Scenario | Requirement ID | Priority |
|-------------|---------------|----------------|----------|
| TS-AUTH-001 | Verify user login with valid credentials | REQ-AUTH-101 | Critical |
| TS-AUTH-002 | Verify user login with invalid username | REQ-AUTH-101 | High |
| TS-AUTH-003 | Verify user login with invalid password | REQ-AUTH-101 | High |
| TS-AUTH-004 | Verify user login with blank credentials | REQ-AUTH-101 | High |
| TS-AUTH-005 | Verify password complexity validation during login | REQ-AUTH-102 | Medium |
| TS-AUTH-006 | Verify account lockout after multiple failed login attempts | REQ-AUTH-103 | Critical |
| TS-AUTH-007 | Verify locked account cannot login with valid credentials | REQ-AUTH-103 | Critical |
| TS-AUTH-008 | Verify password reset functionality | REQ-AUTH-104 | High |
| TS-AUTH-009 | Verify password reset email delivery | REQ-AUTH-104 | High |
| TS-AUTH-010 | Verify session timeout after inactivity | REQ-AUTH-105 | High |
| TS-AUTH-011 | Verify user logout functionality | REQ-AUTH-106 | Critical |
| TS-AUTH-012 | Verify "Remember Me" functionality | REQ-AUTH-107 | Medium |
| TS-AUTH-013 | Verify login with special characters in credentials | REQ-AUTH-101 | Medium |
| TS-AUTH-014 | Verify SQL injection prevention in login form | REQ-AUTH-108 | Critical |

---

### 4.7 Additional Example: E-Commerce Shopping Cart

**Context:** E-Commerce Application - Shopping Cart Feature

**Test Scenarios:**

| Scenario ID | Test Scenario | Priority |
|-------------|---------------|----------|
| TS-CART-001 | Verify adding product to empty cart | Critical |
| TS-CART-002 | Verify adding multiple products to cart | High |
| TS-CART-003 | Verify adding same product multiple times | High |
| TS-CART-004 | Verify removing product from cart | Critical |
| TS-CART-005 | Verify updating product quantity in cart | High |
| TS-CART-006 | Verify cart total calculation | Critical |
| TS-CART-007 | Verify cart persistence across sessions | High |
| TS-CART-008 | Verify cart with maximum allowed items | Medium |
| TS-CART-009 | Verify cart with out-of-stock product | High |
| TS-CART-010 | Verify applying discount code to cart | High |
| TS-CART-011 | Verify clearing entire cart | Medium |
| TS-CART-012 | Verify cart behavior when product price changes | Medium |

---

### 4.8 Test Scenario vs Test Case: Conceptual Comparison

**Relationship:**
- One test scenario typically expands into **multiple test cases**
- Test scenarios define **WHAT** to test
- Test cases define **HOW** to test

**Example Expansion:**

**Test Scenario:** TS-AUTH-001 - Verify user login with valid credentials

**Derived Test Cases:**
- TC-AUTH-001: Login with valid username and valid password
- TC-AUTH-002: Login with valid email and valid password
- TC-AUTH-003: Login with username in uppercase
- TC-AUTH-004: Login with username with leading/trailing spaces
- TC-AUTH-005: Login and verify dashboard landing page
- TC-AUTH-006: Login and verify session creation

**Visual Representation:**
```
Requirement (REQ-AUTH-101)
    ↓
Test Scenario (TS-AUTH-001: Verify login with valid credentials)
    ↓
    ├── Test Case 1: Standard valid login
    ├── Test Case 2: Login with alternate format
    ├── Test Case 3: Login with boundary values
    └── Test Case 4: Login and verify post-login state
```

---

### 4.9 Common Mistakes in Writing Test Scenarios

| Mistake | Example | Correction |
|---------|---------|------------|
| **Too Detailed** | "Login with username 'admin' and password 'Pass@123'" | "Verify user login with valid credentials" |
| **Too Vague** | "Test login" | "Verify user login with valid credentials" |
| **Multiple Scenarios in One** | "Login and add to cart and checkout" | Split into separate scenarios |
| **Non-Testable** | "System should be user-friendly" | "Verify login page loads within 2 seconds" |
| **Technical Focus** | "Verify authentication API returns 200 status" | "Verify user successfully logs in" |
| **No Clear Outcome** | "User enters password" | "Verify login fails with incorrect password" |

---

## 5️⃣ TEST CASES — EXECUTABLE SPECIFICATION

### 5.1 Definition

**A Test Case** is a detailed, step-by-step specification that defines the exact conditions, inputs, actions, and expected results required to verify a specific aspect of system functionality. Test cases are executable specifications that guide testers through the validation process.

**Key Characteristics:**
- **Detailed:** Includes specific steps, data, and conditions
- **Step-Wise:** Breaks down testing into sequential actions
- **Verifiable:** Has measurable and observable expected results
- **Traceable:** Linked to requirements and test scenarios
- **Repeatable:** Can be executed consistently by different testers
- **Documented:** Formal specification with standardized format
- **Executable:** Ready for immediate test execution

**Fundamental Principle:** A test case must be so detailed that any qualified tester, without prior knowledge of the feature, can execute it and determine pass/fail unambiguously.

---

### 5.2 Purpose and Benefits

**Why Test Cases Are Essential:**

1. **Execution Blueprint:** Provides exact instructions for testing
2. **Consistency:** Ensures uniform testing across team members and cycles
3. **Evidence Documentation:** Creates audit trail of testing activities
4. **Defect Reproduction:** Enables precise defect replication
5. **Regression Testing:** Facilitates retesting after changes
6. **Knowledge Repository:** Captures institutional testing knowledge
7. **Coverage Measurement:** Enables quantitative coverage metrics
8. **Training Tool:** Helps new testers understand system behavior
9. **Compliance:** Meets regulatory and audit requirements
10. **Communication:** Clarifies expected system behavior for all stakeholders

---

### 5.3 Mandatory Fields in a Test Case

A well-structured test case must contain the following mandatory fields:

#### **1. Test Case ID**

**Purpose:** Unique identifier for traceability and reference

**Format Convention:**
```
TC-<Module>-<Number>
Example: TC-AUTH-001, TC-CART-045
```

**Best Practices:**
- Use consistent naming convention across project
- Include module/feature prefix for organization
- Use sequential numbering
- Consider version control (e.g., TC-AUTH-001-v2)

---

#### **2. Test Case Description / Title**

**Purpose:** Concise summary of what the test case validates

**Characteristics:**
- Clear and specific
- Action-oriented
- Describes the test objective

**Examples:**
- ✓ "Verify successful login with valid username and password"
- ✓ "Verify error message for login with invalid email format"
- ✗ "Login test" (too vague)
- ✗ "Test to check if the user can successfully login to the system after entering correct credentials" (too verbose)

---

#### **3. Test Scenario ID / Reference**

**Purpose:** Links test case to parent test scenario and requirement

**Example:**
```
Test Scenario: TS-AUTH-001
Requirement ID: REQ-AUTH-101
```

This creates traceability: Requirement → Scenario → Test Case

---

#### **4. Preconditions / Prerequisites**

**Purpose:** Defines the state of the system before test execution begins

**Components:**
- System state requirements
- Data setup requirements
- Environment prerequisites
- User state or permissions

**Examples:**
```
Preconditions:
- Application is accessible via browser
- Test environment is up and running
- Test user account exists in database (username: testuser01, password: Test@123)
- User is NOT currently logged in
- Database contains at least 100 active user records
```

**Why Critical:** Without proper preconditions, test execution may fail due to setup issues rather than actual defects.

---

#### **5. Test Steps / Test Procedure**

**Purpose:** Detailed, sequential instructions for executing the test

**Characteristics:**
- Numbered steps in logical order
- One action per step
- Clear and unambiguous instructions
- Observable actions only

**Format:**
```
Step # | Action
-------|-------
1      | Navigate to application URL: https://app.example.com
2      | Locate the "Username" field
3      | Enter username: "testuser01"
4      | Locate the "Password" field
5      | Enter password: "Test@123"
6      | Click the "Login" button
7      | Observe the resulting page
```

**Best Practices:**
- Use imperative verbs: Navigate, Enter, Click, Select, Verify
- Be specific about UI elements (field names, button labels)
- Include wait/observation steps where timing matters
- Number steps sequentially
- Keep steps atomic (single action each)

---

#### **6. Test Data**

**Purpose:** Specifies exact input values to be used in test execution

**Why Separate from Test Steps:** Allows easy test data variation without rewriting steps

**Examples:**
```
Test Data:
Field: Username
Value: testuser01
Type: Valid

Field: Password
Value: Test@123
Type: Valid, meets complexity requirements (8 chars, uppercase, lowercase, number, special char)

Field: Expected Session Duration
Value: 15 minutes
```

**Complex Test Data Example (E-Commerce):**
```
Product Details:
- Product ID: PROD-12345
- Product Name: "Wireless Mouse"
- Price: $29.99
- Quantity Available: 50
- Discount Code: SAVE10 (10% off)
```

---

#### **7. Expected Result / Expected Outcome**

**Purpose:** Defines the anticipated system behavior if functionality works correctly

**Characteristics:**
- Specific and measurable
- Observable and verifiable
- Directly related to test objective
- Includes all relevant system responses

**Examples:**

**Basic:**
```
Expected Result:
- User is redirected to Dashboard page
- Welcome message displays: "Welcome, testuser01"
- Logout button is visible in top-right corner
```

**Detailed:**
```
Expected Results:
1. HTTP Response: 200 OK
2. Page URL changes to: https://app.example.com/dashboard
3. Page title displays: "Dashboard - E-Banking"
4. User's full name appears in header: "John Doe"
5. Account summary section shows user's accounts
6. Session cookie "AUTH_TOKEN" is created (verify in browser developer tools)
7. Last login timestamp is updated in database
8. Login attempt logged in audit log with status "SUCCESS"
```

**Why Detailed Expected Results Matter:**
- Eliminates ambiguity in pass/fail determination
- Ensures comprehensive validation
- Provides clear acceptance criteria
- Facilitates defect reporting when actual ≠ expected

---

#### **8. Actual Result**

**Purpose:** Documents the observed system behavior during test execution

**Populated During:** Test execution phase

**Values:**
- Actual observed behavior (filled in by tester)
- May match or differ from expected result
- Should be as detailed as expected result

**Examples:**

**When Test Passes:**
```
Actual Result:
- User successfully redirected to Dashboard page
- Welcome message displays: "Welcome, testuser01"
- Logout button visible in top-right corner
- All expected results observed
```

**When Test Fails:**
```
Actual Result:
- User redirected to Dashboard page ✓
- Welcome message displays: "Welcome, null" ✗ (shows "null" instead of username)
- Logout button visible ✓
- Page load time: 5 seconds ✗ (exceeds 2-second requirement)
```

---

#### **9. Status / Execution Status**

**Purpose:** Indicates current state of test case execution

**Standard Values:**
- **Not Executed / Pending:** Test not yet run
- **Pass:** All expected results matched actual results
- **Fail:** One or more expected results did not match actual results
- **Blocked:** Cannot execute due to dependency or environment issue
- **Skipped:** Intentionally not executed (e.g., out of scope for cycle)
- **In Progress:** Currently being executed
- **Needs Retest:** Failed previously, awaiting retesting after fix

**Additional Fields (Often Included):**
- Executed By: Tester name
- Execution Date: Date of test execution
- Build/Version: Software version tested
- Defect ID: Reference to logged defect (if failed)
- Comments/Remarks: Additional notes

---

### 5.4 Complete Test Case Example

**Test Case Specification: User Login with Valid Credentials**

| **Field** | **Value** |
|-----------|-----------|
| **Test Case ID** | TC-AUTH-001 |
| **Test Case Title** | Verify successful user login with valid username and password |
| **Test Scenario ID** | TS-AUTH-001 |
| **Requirement ID** | REQ-AUTH-101 |
| **Test Priority** | Critical |
| **Test Type** | Functional - Positive |
| **Test Designed By** | Maria Rodriguez |
| **Test Design Date** | January 15, 2026 |

---

**Preconditions:**
1. E-Banking application is deployed and accessible at https://ebank.example.com
2. Test user account exists in database:
   - Username: testuser01
   - Password: Test@123
   - Status: Active
   - Role: Standard User
3. Browser (Chrome v120 or later) is installed and launched
4. User is not currently logged in (no active session)
5. Test database is in a known stable state

---

**Test Steps:**

| **Step #** | **Action** | **Expected Behavior** |
|------------|------------|-----------------------|
| 1 | Open browser and navigate to https://ebank.example.com | Login page loads successfully |
| 2 | Verify page title displays "E-Banking - Login" | Page title is correct |
| 3 | Locate the "Username" text input field | Field is visible and enabled |
| 4 | Enter username: `testuser01` | Text appears in field |
| 5 | Locate the "Password" text input field | Field is visible and enabled |
| 6 | Enter password: `Test@123` | Text appears as masked characters (•••) |
| 7 | Locate and click the "Login" button | Button is clickable |
| 8 | Wait for page to load | Page navigation occurs |
| 9 | Observe the resulting page URL | URL changes to https://ebank.example.com/dashboard |
| 10 | Observe page title | Page title displays "E-Banking - Dashboard" |
| 11 | Observe welcome message | Message displays "Welcome, Test User" |
| 12 | Verify logout button presence | "Logout" button visible in top-right corner |
| 13 | Verify user menu displays username | User menu shows "testuser01" |

---

**Test Data:**

| **Field** | **Value** | **Type** | **Notes** |
|-----------|-----------|----------|-----------|
| Username | testuser01 | Valid | Existing active user |
| Password | Test@123 | Valid | Meets complexity: 8 chars, uppercase, lowercase, digit, special |
| Expected Landing Page | /dashboard | Valid | Default page after login |

---

**Expected Results:**
1. Login page loads within 2 seconds
2. Username and password fields accept input
3. After clicking Login button:
   - Page redirects to Dashboard (URL: https://ebank.example.com/dashboard)
   - HTTP response code: 200 OK
   - Page title: "E-Banking - Dashboard"
   - Welcome message displays: "Welcome, Test User" (user's display name)
   - Logout button is visible and enabled
   - User menu displays username: "testuser01"
   - Session cookie "EBANK_SESSION" is created (HttpOnly, Secure flags set)
4. Database updates:
   - `users.last_login_timestamp` updated to current timestamp
   - Entry created in `audit_log` table with action="LOGIN_SUCCESS"
5. No error messages displayed
6. No JavaScript console errors

---

**Actual Results:**
```
[To be filled during test execution]
Executed By: _________________
Execution Date: _________________
Build Version: _________________
```

---

**Status:** Not Executed

---

**Post-Conditions:**
1. User session is active
2. User is authenticated and authorized to access Dashboard
3. Session will expire after 15 minutes of inactivity (per REQ-AUTH-105)

---

**Dependencies:**
- Database server must be running
- SMTP server must be available (for password reset emails, if tested later)
- Application server must be operational

---

**Test Environment:**
- OS: Windows 10 Pro
- Browser: Chrome 120.0
- Test Environment: QA Environment (ebank-qa.example.com)
- Database: PostgreSQL 14.5 (Test instance)

---

**Remarks / Comments:**
This is a critical path test case. Execute in every test cycle. Forms the basis for subsequent authentication tests.

---

### 5.5 Additional Test Case Examples

#### **Example 2: Negative Test Case**

| **Field** | **Value** |
|-----------|-----------|
| **Test Case ID** | TC-AUTH-015 |
| **Test Case Title** | Verify login fails with invalid password and appropriate error message is displayed |
| **Test Scenario ID** | TS-AUTH-003 |
| **Requirement ID** | REQ-AUTH-101 |
| **Test Type** | Functional - Negative |

**Preconditions:**
- Application accessible
- Valid test user exists (username: testuser01, correct password: Test@123)
- User not logged in

**Test Steps:**

| **Step #** | **Action** |
|------------|------------|
| 1 | Navigate to https://ebank.example.com |
| 2 | Enter username: `testuser01` |
| 3 | Enter password: `WrongPass123` (incorrect password) |
| 4 | Click "Login" button |
| 5 | Observe the page response |

**Test Data:**
- Username: testuser01 (Valid)
- Password: WrongPass123 (Invalid - does not match correct password)

**Expected Results:**
1. Login page remains displayed (no redirection)
2. Error message appears: "Invalid username or password. Please try again."
3. Error message displayed in red color near login form
4. Username field retains entered value: "testuser01"
5. Password field is cleared
6. No session cookie created
7. Failed login attempt logged in audit log with status "FAILURE_WRONG_PASSWORD"
8. `users.failed_login_attempts` counter incremented by 1
9. User account remains active (not locked unless this is 3rd consecutive failure)

**Actual Results:** [To be filled]

**Status:** Not Executed

---

#### **Example 3: Boundary Value Test Case**

| **Field** | **Value** |
|-----------|-----------|
| **Test Case ID** | TC-AUTH-032 |
| **Test Case Title** | Verify account lockout on exactly 3rd consecutive failed login attempt |
| **Test Scenario ID** | TS-AUTH-006 |
| **Requirement ID** | REQ-AUTH-103 |
| **Test Type** | Functional - Boundary Value |

**Preconditions:**
- Test user account exists (username: testuser02, password: Test@123)
- Account status: Active
- Failed login attempts counter: 0
- Account is NOT locked

**Test Steps:**

| **Step #** | **Action** |
|------------|------------|
| 1 | Navigate to login page |
| 2 | Enter username: `testuser02`, password: `Wrong1` |
| 3 | Click Login button |
| 4 | Observe error message (1st failure) |
| 5 | Enter username: `testuser02`, password: `Wrong2` |
| 6 | Click Login button |
| 7 | Observe error message (2nd failure) |
| 8 | Enter username: `testuser02`, password: `Wrong3` |
| 9 | Click Login button |
| 10 | Observe error message and account status (3rd failure - CRITICAL) |
| 11 | Verify database: Query `users` table for testuser02 account status |
| 12 | Attempt to login with CORRECT password: `Test@123` |
| 13 | Observe system response |

**Test Data:**
- Username: testuser02
- Correct Password: Test@123
- Invalid Passwords: Wrong1, Wrong2, Wrong3
- Lockout Threshold: 3 consecutive failures (per REQ-AUTH-103)

**Expected Results:**

After 1st failure:
- Error: "Invalid credentials"
- `failed_login_attempts` = 1

After 2nd failure:
- Error: "Invalid credentials"
- `failed_login_attempts` = 2

After 3rd failure (CRITICAL BOUNDARY):
- Error: "Account has been locked due to multiple failed login attempts. Please contact support or reset your password."
- `failed_login_attempts` = 3
- `account_status` = "LOCKED"
- `locked_timestamp` = current timestamp
- Audit log entry: "ACCOUNT_LOCKED"
- Email notification sent to user's registered email

Attempt with correct password after lockout:
- Login FAILS even with correct password
- Error: "Account is locked. Please contact support."
- No authentication attempt (password not even validated)

**Actual Results:** [To be filled]

**Status:** Not Executed

---

### 5.6 Test Case Design Best Practices

#### **1. Independence**
Each test case should be self-contained and executable independently

**Poor Design:**
```
TC-001: Login and add product to cart
TC-002: Continue from previous test and proceed to checkout
```

**Good Design:**
```
TC-001: Login with valid credentials
TC-002: Add product to cart (Precondition: User logged in)
TC-003: Proceed to checkout (Precondition: Cart has 1 product)
```

#### **2. Atomic Focus**
Test one thing at a time

**Poor:** "Verify login, profile update, and logout"

**Good:**
- TC-001: Verify login
- TC-002: Verify profile update
- TC-003: Verify logout

#### **3. Clear Pass/Fail Criteria**
Expected results must be unambiguous

**Poor:** "System should work properly"

**Good:** "Dashboard page loads within 2 seconds with HTTP 200 status"

#### **4. Maintainability**
Design for long-term maintenance

- Use parameterized test data tables
- Avoid hardcoding environment-specific values
- Document assumptions
- Version control test cases

#### **5. Traceability**
Every test case must trace to:
- Requirement ID
- Test Scenario ID
- Potentially: User Story, Design Document

---

## 6️⃣ POSITIVE TEST CASES

### 6.1 Definition

**Positive Test Cases** validate that the system behaves correctly when provided with valid inputs and under expected conditions. They verify the "happy path" - the ideal flow when everything goes right.

**Objective:** Confirm that the system meets functional requirements and delivers expected features when used correctly.

**Fundamental Principle:** Positive testing answers "Does the system do what it's supposed to do?"

---

### 6.2 Characteristics

| Characteristic | Description |
|---------------|-------------|
| **Valid Input Focus** | Uses legitimate, acceptable input values |
| **Expected Behavior** | Tests normal, intended system
